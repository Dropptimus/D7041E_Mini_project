{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.cluster import KMeans, AffinityPropagation\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from clustering import clustering_classification, test_classifier, write, save_metrics_to_dict, encode_categorical_features, import_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For logging "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = SummaryWriter()\n",
    "\n",
    "train_acc_dict = {}\n",
    "train_f1_dict = {}\n",
    "test_acc_dict = {}\n",
    "test_f1_dict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Line magic function `%tensorboard` not found.\n"
     ]
    }
   ],
   "source": [
    "# Launch tensorboard\n",
    "# images=21 change this to how many datasets you use\n",
    "%tensorboard --logdir runs/ --port 6006 --samples_per_plugin images=21\n",
    "# If in use (Mac) use to find the process PID\n",
    "% lsof -i :6006\n",
    "# Kill the process with \n",
    "% kill -9 <PID>\n",
    "# Then launch using bash with first command"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Config Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_SIZE = 0.2\n",
    "RANDOM_SEED = 42\n",
    "K_FOLDS = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Our chosen datasets\n",
    "TODO\n",
    "We have chosen the datasets based on the following criteria:\n",
    "- datasets where we do not need to do any special preprocessing so that it is easy to do in only one pipeline\n",
    "- rather small datasets to ensure we do not need high computational power"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'abalone': (1, 1), 'acute_inflamations': (184, 2), 'balance_scale': (12, 3), 'balloons': (13, 4), 'breast_cancer_wisconsin_diagnostic': (17, 5), 'car_evaluation': (19, 6), 'congress_voting_records': (105, 7), 'credit_approval': (27, 8), 'ecoli': (39, 9), 'fertility': (244, 10), 'habermans_survival': (43, 11), 'hayes_roth': (44, 12), 'heart_disease': (45, 13), 'ilpd': (225, 14), 'iris': (53, 15), 'lenses': (58, 16), 'mammographic_mass': (161, 17), 'mushroom': (73, 18), 'statlog': (144, 19), 'wine_quality': (186, 20), 'zoo': (111, 21)}\n"
     ]
    }
   ],
   "source": [
    "# Dataset that seems useful\n",
    "dataset_id = {\n",
    "    \"iris\": 53, \n",
    "    \"heart_disease\": 45, \n",
    "    \"wine_quality\": 186, \n",
    "    \"breast_cancer_wisconsin_diagnostic\": 17, \n",
    "    \"car_evaluation\": 19, \n",
    "    # \"abalone\": 1, \n",
    "    \"mushroom\": 73, \n",
    "    \"statlog\" : 144, \n",
    "    \"credit_approval\" : 27, \n",
    "    \"zoo\" : 111, \n",
    "    \"balance_scale\" : 12, \n",
    "    \"ilpd\" : 225, \n",
    "    \"acute_inflamations\" : 184, \n",
    "    \"ecoli\" : 39, \n",
    "    \"mammographic_mass\" : 161, \n",
    "    \"hayes_roth\" : 44, \n",
    "    \"habermans_survival\" : 43, \n",
    "    \"congress_voting_records\" : 105, \n",
    "    \"balloons\" : 13, \n",
    "    \"lenses\" : 58, \n",
    "    \"fertility\" : 244, \n",
    "}\n",
    "\n",
    "data_set_sorted = {}\n",
    "for i, name in enumerate(sorted(dataset_id.keys())):\n",
    "    data_set_sorted[name] = (dataset_id[name], i+1)\n",
    "\n",
    "print(data_set_sorted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import and preprocess datasets\n",
    "For the preprocessing we will do the following steps:\n",
    "1. Remove any missing values. In the article the following is written: \"Given that our classifiers are not oriented to data with missing features, the missing inputs are treated as zero, which should not bias the comparison results.\" We therefore also decided to just remove missing values and to more focus on the full pipeline instead of single datasets. Another way could have been interpolation.\n",
    "2. Encode categorical data into numerical data. This we have to do to use the classifiers later on.\n",
    "3. Remove certain columns if they are highly correlated to others.\n",
    "4. Split the data into a train and a test set. We will use a 80/20 split.\n",
    "5. Scale the data so that we have zero mean and standard deviation of one. This is done with the Standard scaler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ordinal_encoder = OrdinalEncoder()\n",
    "# X, y = import_dataset(dataset_id[\"iris\"], ordinal_encoder)\n",
    "# y = encode_categorical_features(y, ordinal_encoder)\n",
    "# # split the dataset\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=TEST_SIZE, random_state=RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing different classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-Nearest Neighbor classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://towardsdatascience.com/gridsearchcv-for-beginners-db48a90114ee\n",
    "knn_params = [{'knn__n_neighbors': [3, 5, 7, 9],\n",
    "        'knn__weights': ['uniform', 'distance'],\n",
    "        'knn__leaf_size': [15, 20]}]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support vector machine classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# params taken from here: https://www.geeksforgeeks.org/svm-hyperparameter-tuning-using-gridsearchcv-ml/\n",
    "svm_params = [{'svm__C': [0.1, 1, 10, 100, 1000],  \n",
    "        'svm__gamma': [1, 0.1, 0.01, 0.001, 0.0001], \n",
    "        'svm__kernel': ['rbf']} ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic regression classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# params taken from here: https://www.geeksforgeeks.org/how-to-optimize-logistic-regression-performance/\n",
    "log_reg_params = [\n",
    "    {'log_reg__penalty':['l1','l2','elasticnet','none'],\n",
    "    #'log_reg__C' : np.logspace(-4,4,10),\n",
    "    'log_reg__solver': ['lbfgs','newton-cg','liblinear','saga'],\n",
    "    #'log_reg__max_iter'  : [100,2500,5000]\n",
    "}\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-means unsupervised classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans_params = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Affinity propagation unsupervised classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "affinity_propagation_params = [{\"damping\": [0.5, 0.7]}, \n",
    "          {\"convergence_iter\": [10, 15]},\n",
    "          #{\"preference\": [0.01]}\n",
    "          ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train test loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "****************************************************************************************************\n",
      "Current dataset: abalone\n",
      "Dataset size: 4177\n",
      "Labels in dataset: [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 25 26 27 29]\n",
      "****************************************************************************************************\n",
      "\n",
      "____________________________________________________________________________________________________\n",
      "Classifier: knn\n",
      "Cross validation best parameters: {'knn__leaf_size': 15, 'knn__n_neighbors': 7, 'knn__weights': 'uniform'}\n",
      "knn, Train accuracy = 0.40077821011673154, Test accuracy = 0.24043062200956938\n",
      "knn, Train f1-score = 0.38253583148275055, Test f1-score = 0.22477344651495762\n",
      "____________________________________________________________________________________________________\n",
      "Classifier: svm\n",
      "Cross validation best parameters: {'svm__C': 100, 'svm__gamma': 0.01, 'svm__kernel': 'rbf'}\n",
      "svm, Train accuracy = 0.29242741694103563, Test accuracy = 0.284688995215311\n",
      "svm, Train f1-score = 0.2633791057773016, Test f1-score = 0.25209235439559435\n",
      "____________________________________________________________________________________________________\n",
      "Classifier: log_reg\n",
      "Cross validation best parameters: {'log_reg__penalty': 'l2', 'log_reg__solver': 'lbfgs'}\n",
      "log_reg, Train accuracy = 0.27985633043998803, Test accuracy = 0.2751196172248804\n",
      "log_reg, Train f1-score = 0.25258312751881545, Test f1-score = 0.24847519308649163\n",
      "____________________________________________________________________________________________________\n",
      "Classifier: kmeans\n",
      "k-means n cluster 28\n",
      "Cross validation best parameters:  {'n_clusters': 28, 'algorithm': 'lloyd'}\n",
      "kmeans, Train accuracy = 0.2723735408560311, Test accuracy = 0.2799043062200957\n",
      "kmeans, Train f1-score = 0.2364741946423434, Test f1-score = 0.2380556661842794\n",
      "____________________________________________________________________________________________________\n",
      "Classifier: affinity_propagation\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 105\u001b[0m\n\u001b[0;32m    103\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m100\u001b[39m)\n\u001b[0;32m    104\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mClassifier: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mclf_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 105\u001b[0m train_acc, train_f1, test_acc, test_f1, cm_train, cm_test \u001b[38;5;241m=\u001b[39m  \u001b[43mclustering_classification\u001b[49m\u001b[43m(\u001b[49m\u001b[43mAffinityPropagation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclf_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maffinity_propagation_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mRANDOM_SEED\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mK_FOLDS\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    106\u001b[0m write(writer, clf_name, cm_train, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mid\u001b[39m[\u001b[38;5;241m1\u001b[39m])\n\u001b[0;32m    107\u001b[0m write(writer, clf_name, cm_test, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mid\u001b[39m[\u001b[38;5;241m1\u001b[39m] )\n",
      "File \u001b[1;32mc:\\Users\\britt\\OneDrive - Luleå University of Technology\\AppliedAI\\MiniProject\\D7041E_Mini_project\\clustering.py:48\u001b[0m, in \u001b[0;36mclustering_classification\u001b[1;34m(ClusteringClass, cls_name, params, X_train, y_train, X_test, y_test, random_seed, k_folds)\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     44\u001b[0m \u001b[38;5;124;03mTest clustering classification on dataset\u001b[39;00m\n\u001b[0;32m     45\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     46\u001b[0m RANDOM_SEED \u001b[38;5;241m=\u001b[39m random_seed\n\u001b[1;32m---> 48\u001b[0m params \u001b[38;5;241m=\u001b[39m \u001b[43mclassification_cv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mClusteringClass\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcls_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk_folds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandom_seed\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     50\u001b[0m \u001b[38;5;66;03m# create classifier\u001b[39;00m\n\u001b[0;32m     51\u001b[0m clf \u001b[38;5;241m=\u001b[39m ClusteringClass(random_state\u001b[38;5;241m=\u001b[39mRANDOM_SEED, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams)\n",
      "File \u001b[1;32mc:\\Users\\britt\\OneDrive - Luleå University of Technology\\AppliedAI\\MiniProject\\D7041E_Mini_project\\clustering.py:114\u001b[0m, in \u001b[0;36mclassification_cv\u001b[1;34m(ClusteringClass, cls_name, params, X_train, y_train, k_folds, random_seed)\u001b[0m\n\u001b[0;32m    111\u001b[0m X_train_cv \u001b[38;5;241m=\u001b[39m scaler_cv\u001b[38;5;241m.\u001b[39mfit_transform(X_train_cv)\n\u001b[0;32m    113\u001b[0m \u001b[38;5;66;03m# Fit and predict training\u001b[39;00m\n\u001b[1;32m--> 114\u001b[0m \u001b[43mclustering_algorithm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train_cv\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    115\u001b[0m cluster_labels_cv \u001b[38;5;241m=\u001b[39m clustering_algorithm\u001b[38;5;241m.\u001b[39mpredict(X_train_cv)\n\u001b[0;32m    117\u001b[0m \u001b[38;5;66;03m# fit and predict validation\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\britt\\anaconda3\\envs\\AppliedAILabs\\Lib\\site-packages\\sklearn\\base.py:1473\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1466\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1468\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1469\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1470\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1471\u001b[0m     )\n\u001b[0;32m   1472\u001b[0m ):\n\u001b[1;32m-> 1473\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\britt\\anaconda3\\envs\\AppliedAILabs\\Lib\\site-packages\\sklearn\\cluster\\_affinity_propagation.py:531\u001b[0m, in \u001b[0;36mAffinityPropagation.fit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    523\u001b[0m preference \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(preference)\n\u001b[0;32m    525\u001b[0m random_state \u001b[38;5;241m=\u001b[39m check_random_state(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrandom_state)\n\u001b[0;32m    527\u001b[0m (\n\u001b[0;32m    528\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcluster_centers_indices_,\n\u001b[0;32m    529\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlabels_,\n\u001b[0;32m    530\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_iter_,\n\u001b[1;32m--> 531\u001b[0m ) \u001b[38;5;241m=\u001b[39m \u001b[43m_affinity_propagation\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    532\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maffinity_matrix_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    533\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_iter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_iter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    534\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconvergence_iter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvergence_iter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    535\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpreference\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpreference\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    536\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdamping\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdamping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    537\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    538\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_n_iter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    539\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrandom_state\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    540\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    542\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maffinity \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprecomputed\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    543\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcluster_centers_ \u001b[38;5;241m=\u001b[39m X[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcluster_centers_indices_]\u001b[38;5;241m.\u001b[39mcopy()\n",
      "File \u001b[1;32mc:\\Users\\britt\\anaconda3\\envs\\AppliedAILabs\\Lib\\site-packages\\sklearn\\cluster\\_affinity_propagation.py:105\u001b[0m, in \u001b[0;36m_affinity_propagation\u001b[1;34m(S, preference, convergence_iter, max_iter, damping, verbose, return_n_iter, random_state)\u001b[0m\n\u001b[0;32m    102\u001b[0m R \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tmp\n\u001b[0;32m    104\u001b[0m \u001b[38;5;66;03m# tmp = Rp; compute availabilities\u001b[39;00m\n\u001b[1;32m--> 105\u001b[0m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmaximum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mR\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtmp\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    106\u001b[0m tmp\u001b[38;5;241m.\u001b[39mflat[:: n_samples \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m=\u001b[39m R\u001b[38;5;241m.\u001b[39mflat[:: n_samples \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m    108\u001b[0m \u001b[38;5;66;03m# tmp = -Anew\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for name, id in data_set_sorted.items():\n",
    "        print(\"\\n\" + \"*\"*100)\n",
    "        print(f\"Current dataset: {name}\")\n",
    "        ordinal_encoder = OrdinalEncoder()\n",
    "\n",
    "        # Set up dataset\n",
    "        X, y = import_dataset(dataset_id[name], ordinal_encoder)\n",
    "        labels = np.unique(y)\n",
    "        y = encode_categorical_features(y, ordinal_encoder)\n",
    "        print(f\"Dataset size: {len(X)}\")\n",
    "        print(f\"Labels in dataset: {labels}\")\n",
    "\n",
    "        # split the dataset\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=TEST_SIZE, random_state=RANDOM_SEED, stratify = y)\n",
    "        print(\"*\"*100, end=\"\\n\\n\")\n",
    "\n",
    "        # KNN\n",
    "        clf_name = \"knn\"\n",
    "        print(\"_\"*100)\n",
    "        print(f\"Classifier: {clf_name}\")\n",
    "        # https://towardsdatascience.com/gridsearchcv-for-beginners-db48a90114ee\n",
    "        knn = KNeighborsClassifier()\n",
    "        train_acc, train_f1, test_acc, test_f1, cm_train, cm_test = test_classifier(knn, clf_name, knn_params, X_train, y_train, X_test, y_test)\n",
    "\n",
    "        write(writer, clf_name, cm_train, \"train\", id[1])\n",
    "        write(writer, clf_name, cm_test, \"test\", id[1])\n",
    "\n",
    "        train_acc_dict, train_f1_dict, test_acc_dict, test_f1_dict = save_metrics_to_dict(clf_name, \n",
    "                                                                                          train_acc, \n",
    "                                                                                          train_f1, \n",
    "                                                                                          test_acc, \n",
    "                                                                                          test_f1, \n",
    "                                                                                          train_acc_dict, \n",
    "                                                                                          train_f1_dict, \n",
    "                                                                                          test_acc_dict, \n",
    "                                                                                          test_f1_dict)\n",
    "        \n",
    "        # SVM \n",
    "        svm = SVC()\n",
    "        clf_name = \"svm\"\n",
    "        print(\"_\"*100)\n",
    "        print(f\"Classifier: {clf_name}\")\n",
    "        train_acc, train_f1, test_acc, test_f1, cm_train, cm_test = test_classifier(svm, clf_name, svm_params, X_train, y_train, X_test, y_test)\n",
    "        write(writer, clf_name, cm_train, \"train\", id[1])\n",
    "        write(writer, clf_name, cm_test, \"test\", id[1] )\n",
    "        train_acc_dict, train_f1_dict, test_acc_dict, test_f1_dict = save_metrics_to_dict(clf_name, \n",
    "                                                                                          train_acc, \n",
    "                                                                                          train_f1, \n",
    "                                                                                          test_acc, \n",
    "                                                                                          test_f1, \n",
    "                                                                                          train_acc_dict, \n",
    "                                                                                          train_f1_dict, \n",
    "                                                                                          test_acc_dict, \n",
    "                                                                                          test_f1_dict)\n",
    "\n",
    "        # Logistic regression\n",
    "        log_reg = LogisticRegression()\n",
    "        clf_name = \"log_reg\"\n",
    "        print(\"_\"*100)\n",
    "        print(f\"Classifier: {clf_name}\")\n",
    "        train_acc, train_f1, test_acc, test_f1, cm_train, cm_test = test_classifier(log_reg, \n",
    "                                                                                    clf_name, \n",
    "                                                                                    log_reg_params, X_train, y_train, X_test, y_test)\n",
    "        write(writer, clf_name, cm_train, \"train\", id[1])\n",
    "        write(writer, clf_name, cm_test, \"test\", id[1] )\n",
    "        train_acc_dict, train_f1_dict, test_acc_dict, test_f1_dict = save_metrics_to_dict(clf_name, \n",
    "                                                                                          train_acc, \n",
    "                                                                                          train_f1, \n",
    "                                                                                          test_acc, \n",
    "                                                                                          test_f1, \n",
    "                                                                                          train_acc_dict, \n",
    "                                                                                          train_f1_dict, \n",
    "                                                                                          test_acc_dict, \n",
    "                                                                                          test_f1_dict)\n",
    "       \n",
    "        # K-means\n",
    "        clf_name = \"kmeans\"\n",
    "        print(\"_\"*100)\n",
    "        print(f\"Classifier: {clf_name}\")\n",
    "        kmeans_params = [{\"algorithm\": [\"lloyd\", \"elkan\"]}]\n",
    "        n_clusters = len(labels)\n",
    "        print(\"k-means n cluster\", n_clusters)\n",
    "        kmeans_params.append({\"n_clusters\": [n_clusters]})\n",
    "        train_acc, train_f1, test_acc, test_f1, cm_train, cm_test = clustering_classification(KMeans, clf_name, kmeans_params, X_train, y_train, X_test, y_test, RANDOM_SEED, K_FOLDS)\n",
    "        write(writer, clf_name, cm_train, \"train\", id[1])\n",
    "        write(writer, clf_name, cm_test, \"test\", id[1] )\n",
    "        train_acc_dict, train_f1_dict, test_acc_dict, test_f1_dict = save_metrics_to_dict(clf_name, \n",
    "                                                                                          train_acc, \n",
    "                                                                                          train_f1, \n",
    "                                                                                          test_acc, \n",
    "                                                                                          test_f1, \n",
    "                                                                                          train_acc_dict, \n",
    "                                                                                          train_f1_dict, \n",
    "                                                                                          test_acc_dict, \n",
    "                                                                                          test_f1_dict)\n",
    "\n",
    "        # Agglomerative clustering\n",
    "\n",
    "        \n",
    "\n",
    "        # Affinity propagation\n",
    "        clf_name = \"affinity_propagation\"\n",
    "        print(\"_\"*100)\n",
    "        print(f\"Classifier: {clf_name}\")\n",
    "        train_acc, train_f1, test_acc, test_f1, cm_train, cm_test =  clustering_classification(AffinityPropagation, clf_name, affinity_propagation_params, X_train, y_train, X_test, y_test, RANDOM_SEED, K_FOLDS)\n",
    "        write(writer, clf_name, cm_train, \"train\", id[1])\n",
    "        write(writer, clf_name, cm_test, \"test\", id[1] )\n",
    "        train_acc_dict, train_f1_dict, test_acc_dict, test_f1_dict = save_metrics_to_dict(clf_name, \n",
    "                                                                                          train_acc, \n",
    "                                                                                          train_f1, \n",
    "                                                                                          test_acc, \n",
    "                                                                                          test_f1, \n",
    "                                                                                          train_acc_dict, \n",
    "                                                                                          train_f1_dict, \n",
    "                                                                                          test_acc_dict, \n",
    "                                                                                          test_f1_dict)\n",
    "\n",
    "        # Write metrics to tensorboard, step is dataset id\n",
    "        writer.add_scalars(\"Train acc\", train_acc_dict, id[1])\n",
    "        writer.add_scalars(\"Test acc\", test_acc_dict, id[1])\n",
    "        writer.add_scalars(\"Train f1\", train_f1_dict, id[1])\n",
    "        writer.add_scalars(\"Test f1\", test_f1_dict, id[1])\n",
    "\n",
    "\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# svm = SVC()\n",
    "# # params taken from here: https://www.geeksforgeeks.org/svm-hyperparameter-tuning-using-gridsearchcv-ml/\n",
    "# params = [{'svm__C': [0.1, 1, 10, 100, 1000],  \n",
    "#               'svm__gamma': [1, 0.1, 0.01, 0.001, 0.0001], \n",
    "#               'svm__kernel': ['rbf']} ]\n",
    "\n",
    "# test_classifier(svm, \"svm\", params, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# log_reg = LogisticRegression()\n",
    "# # params taken from here: https://www.geeksforgeeks.org/how-to-optimize-logistic-regression-performance/\n",
    "# params = [\n",
    "#     {'log_reg__penalty':['l1','l2','elasticnet','none'],\n",
    "#     #'log_reg__C' : np.logspace(-4,4,10),\n",
    "#     'log_reg__solver': ['lbfgs','newton-cg','liblinear','saga'],\n",
    "#     'log_reg__max_iter'  : [100,2500,5000]\n",
    "# }\n",
    "# ]\n",
    "\n",
    "# test_classifier(log_reg, \"log_reg\", params, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-means clustering classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n_clusters = len(np.unique(y_train))\n",
    "# params = [{\"n_clusters\": [n_clusters, int(n_clusters/2)]}]\n",
    "# clf_name = \"kmeans\"\n",
    "\n",
    "# train_acc, train_f1, test_acc, test_f1 = clustering_classification(KMeans, clf_name, params, X_train, y_train, X_test, y_test, RANDOM_SEED, K_FOLDS)\n",
    "# train_acc_dict[clf_name] = train_acc\n",
    "# train_f1_dict[clf_name] = train_f1\n",
    "# test_acc_dict[clf_name] = test_acc\n",
    "# test_f1_dict[clf_name] = test_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# params = [{\"damping\": [0.5, 0.99]}, \n",
    "#           {\"convergence_iter\": [10, 15]},\n",
    "#           #{\"preference\": [0.01]}\n",
    "#           ]\n",
    "# clf_name = \"affinity_propagation\"\n",
    "\n",
    "# clustering_classification(AffinityPropagation, clf_name, params, X_train, y_train, X_test, y_test, RANDOM_SEED, K_FOLDS)\n",
    "# train_acc_dict[clf_name] = train_acc\n",
    "# train_f1_dict[clf_name] = train_f1\n",
    "# test_acc_dict[clf_name] = test_acc\n",
    "# test_f1_dict[clf_name] = test_f1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(train_acc_dict)\n",
    "# print(test_acc_dict)\n",
    "# print(train_f1_dict)\n",
    "# print(test_f1_dict)\n",
    "\n",
    "# writer.add_scalars(\"Train acc\", train_acc_dict)\n",
    "# writer.add_scalars(\"Test acc\", test_acc_dict)\n",
    "# writer.add_scalars(\"Train f1\", train_f1_dict)\n",
    "# writer.add_scalars(\"Test f1\", test_f1_dict)\n",
    "# writer.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AppliedAILabs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
