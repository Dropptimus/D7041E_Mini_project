{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ucimlrepo import fetch_ucirepo, list_available_datasets\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder, OrdinalEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Config Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_SIZE = 0.2\n",
    "RANDOM_SEED = 42\n",
    "K_FOLDS = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Our chosen datasets\n",
    "TODO\n",
    "We have chosen the datasets based on the following criteria:\n",
    "- datasets where we do not need to do any special preprocessing so that it is easy to do in only one pipeline\n",
    "- rather small datasets to ensure we do not need high computational power"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset that seems useful\n",
    "dataset_id = {\"iris\": 53, \n",
    "              \"heart_disease\": 45,\n",
    "              \"wine_quality\": 186,\n",
    "              \"breast_cancer_wisconsin\": 17,\n",
    "              \"car_evaluation\": 19, # Not good maybe remove\n",
    "              \"abalone\": 1,\n",
    "              \"mushroom\": 73, # a lot of categorical data\n",
    "              \"statlog\" : 144, # german credit\n",
    "              \"student_performance\" : 320,\n",
    "              \"accute_inflammation\" : 284,\n",
    "              \"credit_approval\" : 143,\n",
    "              \"wholesale_customers\" : 292,\n",
    "              \"glass_identifcation\" : 42,\n",
    "              \"ilpd\" : 225,\n",
    "              \"hcv\" : 503,\n",
    "              \"land_mines\" : 763,\n",
    "              \"balance_scale\" : 12\n",
    "              }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import and preprocess datasets\n",
    "For the preprocessing we will do the following steps:\n",
    "1. Remove any missing values. In the article the following is written: \"Given that our classifiers are not oriented to data with missing features, the missing inputs are treated as zero, which should not bias the comparison results.\" We therefore also decided to just remove missing values and to more focus on the full pipeline instead of single datasets. Another way could have been interpolation.\n",
    "2. Encode categorical data into numerical data. This we have to do to use the classifiers later on.\n",
    "3. Remove certain columns if they are highly correlated to others.\n",
    "4. Split the data into a train and a test set. We will use a 80/20 split.\n",
    "5. Scale the data so that we have zero mean and standard deviation of one. This is done with the Standard scaler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for converting categorical features into numerical\n",
    "def encode_categorical_features(X, encoder):\n",
    "    X = encoder.fit_transform(X)\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_dataset(uci_id, encoder):\n",
    "    # get the dataset\n",
    "    dataset = fetch_ucirepo(id=uci_id) \n",
    "    # load data into dataframe for easier preprocessing\n",
    "    df = pd.concat([dataset[\"data\"][\"features\"],dataset[\"data\"][\"targets\"] ],axis=1)\n",
    "    # remove nan values\n",
    "    df.dropna(inplace=True)\n",
    "    \n",
    "    X = df.iloc[:,:-1]\n",
    "    y = df.iloc[:,-1:]\n",
    "    # encode categorical data only for features not for target itself\n",
    "    # https://stackoverflow.com/questions/29803093/check-which-columns-in-dataframe-are-categorical\n",
    "    cols = X.columns\n",
    "    num_cols = X._get_numeric_data().columns\n",
    "    #print(num_cols)\n",
    "    categorical_cols = list(set(cols) - set(num_cols))\n",
    "    #print(categorical_cols)\n",
    "    X.loc[:, categorical_cols] = encode_categorical_features(X[categorical_cols], encoder)\n",
    "    \n",
    "    # check if encoding has worked\n",
    "    # https://stackoverflow.com/questions/26924904/check-if-dataframe-column-is-categorical\n",
    "    for c in X.columns:\n",
    "        if X[c].dtype.name == \"category\":\n",
    "            print(f\"WARNING: Column {c} still has categorical values!\")\n",
    "            \n",
    "    # last column is target\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Diagnosis\n",
      "0           M\n",
      "1           M\n",
      "2           M\n",
      "3           M\n",
      "4           M\n",
      "..        ...\n",
      "564         M\n",
      "565         M\n",
      "566         M\n",
      "567         M\n",
      "568         B\n",
      "\n",
      "[569 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "ordinal_encoder = OrdinalEncoder()\n",
    "X, y = import_dataset(dataset_id[\"breast_cancer_wisconsin\"], ordinal_encoder)\n",
    "print(y)\n",
    "#print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=TEST_SIZE, random_state=RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.974 ,\n",
      "Standard Deviations :0.018\n"
     ]
    }
   ],
   "source": [
    "log_reg = LogisticRegression(max_iter=1000)\n",
    "\n",
    "# Ravel to convert from (len, 1) shape to (len,), warning from sk-learn\n",
    "Y_train = np.ravel(y_train)\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X_train)\n",
    "\n",
    "scores = cross_val_score(\n",
    "    log_reg, X_scaled, Y_train, scoring='accuracy', cv=5)\n",
    "# accuracy\n",
    "print('Accuracy: %.3f ,\\nStandard Deviations :%.3f' %\n",
    "      (np.mean(scores), np.std(scores)))\n",
    "\n",
    "#TODO check if this is better implementation\n",
    "# k_folds  = KFold(n_splits=K_FOLDS)\n",
    "# for train_idx, valid_idx in k_folds.split(X_train):\n",
    "#     continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'knn__leaf_size': 15, 'knn__n_neighbors': 5, 'knn__weights': 'uniform'}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9802197802197802"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# https://towardsdatascience.com/gridsearchcv-for-beginners-db48a90114ee\n",
    "pipe = Pipeline([('scaler', StandardScaler()), ('knn', KNeighborsClassifier())])\n",
    "params = [{'knn__n_neighbors': [3, 5, 7, 9],\n",
    "         'knn__weights': ['uniform', 'distance'],\n",
    "         'knn__leaf_size': [15, 20]}]\n",
    "gs_knn = GridSearchCV(pipe,\n",
    "                      param_grid=params,\n",
    "                      scoring='accuracy',\n",
    "                      cv=5)\n",
    "\n",
    "# Ravel to convert from (len, 1) shape to (len,), warning from sk-learn\n",
    "y_train = np.ravel(y_train)\n",
    "\n",
    "gs_knn.fit(X_train, y_train)\n",
    "print(gs_knn.best_params_)\n",
    "# find best model score\n",
    "gs_knn.score(X_train, y_train)\n",
    "\n",
    "# test on test set\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AppliedAILabs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
