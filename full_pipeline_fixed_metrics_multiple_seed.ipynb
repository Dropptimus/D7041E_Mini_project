{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.cluster import KMeans, AffinityPropagation\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from utils import clustering_classification, test_classifier, write, save_metrics_to_dict, encode_categorical_features, import_dataset, agg_clustering, writer_add_scalars"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For logging "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = SummaryWriter(log_dir=\"C:\\\\awilde\\\\britta\\\\LTU\\\\AppliedAI\\\\runs\")\n",
    "#writer = SummaryWriter()\n",
    "metrics_dict={\n",
    "\"train_acc_dict\" : {},\n",
    "\"train_f1_dict\" : {},\n",
    "\"test_acc_dict\" : {},\n",
    "\"test_f1_dict\" : {},\n",
    "\"train_acc_avg\" : {},\n",
    "\"train_f1_avg\" : {},\n",
    "\"test_acc_avg\" : {},\n",
    "\"test_f1_avg\" : {},\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Line magic function `%tensorboard` not found.\n"
     ]
    }
   ],
   "source": [
    "# Launch tensorboard\n",
    "# images=21 change this to how many datasets you use\n",
    "%tensorboard --logdir runs/ --port 6006 --samples_per_plugin images=22\n",
    "# If in use (Mac) use to find the process PID\n",
    "% lsof -i :6006\n",
    "# Kill the process with \n",
    "% kill -9 <PID>\n",
    "# Then launch using bash with first command"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Config Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_SIZE = 0.2\n",
    "RANDOM_SEEDS = [41, 42, 43, 44, 45]\n",
    "K_FOLDS = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Our chosen datasets\n",
    "TODO\n",
    "We have chosen the datasets based on the following criteria:\n",
    "- datasets where we do not need to do any special preprocessing so that it is easy to do in only one pipeline\n",
    "- rather small datasets to ensure we do not need high computational power"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'acute_inflamations': (184, 1), 'balance_scale': (12, 2), 'balloons': (13, 3), 'breast_cancer_wisconsin_diagnostic': (17, 4), 'car_evaluation': (19, 5), 'congress_voting_records': (105, 6), 'credit_approval': (27, 7), 'ecoli': (39, 8), 'fertility': (244, 9), 'habermans_survival': (43, 10), 'hayes_roth': (44, 11), 'heart_disease': (45, 12), 'ilpd': (225, 13), 'iris': (53, 14), 'lenses': (58, 15), 'mammographic_mass': (161, 16), 'mushroom': (73, 17), 'spect_heart': (95, 18), 'spectf_heart': (96, 19), 'statlog': (144, 20), 'wine_quality': (186, 21), 'zoo': (111, 22)}\n"
     ]
    }
   ],
   "source": [
    "# Dataset that seems useful\n",
    "dataset_id = {\n",
    "    \"iris\": 53, \n",
    "    \"heart_disease\": 45, \n",
    "    \"wine_quality\": 186, \n",
    "    \"breast_cancer_wisconsin_diagnostic\": 17, \n",
    "    \"car_evaluation\": 19, \n",
    "    \"spect_heart\" : 95, \n",
    "    \"spectf_heart\" : 96,\n",
    "    \"mushroom\": 73, \n",
    "    \"statlog\" : 144, \n",
    "    \"credit_approval\" : 27, \n",
    "    \"zoo\" : 111, \n",
    "    \"balance_scale\" : 12, \n",
    "    \"ilpd\" : 225, \n",
    "    \"acute_inflamations\" : 184, \n",
    "    \"ecoli\" : 39, \n",
    "    \"mammographic_mass\" : 161, \n",
    "    \"hayes_roth\" : 44, \n",
    "    \"habermans_survival\" : 43, \n",
    "    \"congress_voting_records\" : 105, \n",
    "    \"balloons\" : 13, \n",
    "    \"lenses\" : 58, \n",
    "    \"fertility\" : 244, \n",
    "}\n",
    "\n",
    "# sort alphabetically and adds id for logging\n",
    "data_set_sorted = {}\n",
    "for i, name in enumerate(sorted(dataset_id.keys())):\n",
    "    data_set_sorted[name] = (dataset_id[name], i+1)\n",
    "\n",
    "print(data_set_sorted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import and preprocess datasets\n",
    "For the preprocessing we will do the following steps:\n",
    "1. Remove any missing values. In the article the following is written: \"Given that our classifiers are not oriented to data with missing features, the missing inputs are treated as zero, which should not bias the comparison results.\" We therefore also decided to just remove missing values and to more focus on the full pipeline instead of single datasets. Another way could have been interpolation.\n",
    "2. Encode categorical data into numerical data. This we have to do to use the classifiers later on.\n",
    "3. Remove certain columns if they are highly correlated to others. <span style=\"color: red;\">ALERT!</span>\n",
    "4. Split the data into a train and a test set. We will use a 80/20 split.\n",
    "5. Scale the data so that we have zero mean and standard deviation of one. This is done with the Standard scaler."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing different classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-Nearest Neighbor classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://towardsdatascience.com/gridsearchcv-for-beginners-db48a90114ee\n",
    "knn_params = [{'knn__n_neighbors': [3, 5, 7, 9],\n",
    "        'knn__weights': ['uniform', 'distance'],\n",
    "        'knn__leaf_size': [15, 20]}]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support vector machine classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# params taken from here: https://www.geeksforgeeks.org/svm-hyperparameter-tuning-using-gridsearchcv-ml/\n",
    "svm_params = [{'svm__C': [0.1, 1, 10, 100, 1000],  \n",
    "        'svm__gamma': [1, 0.1, 0.01, 0.001, 0.0001], \n",
    "        'svm__kernel': ['rbf']} ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic regression classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# params taken from here: https://www.geeksforgeeks.org/how-to-optimize-logistic-regression-performance/\n",
    "# and from here https://www.kaggle.com/code/enespolat/grid-search-with-logistic-regression\n",
    "log_reg_params = [\n",
    "    {'log_reg__penalty':['l1','l2'],\n",
    "    'log_reg__C' : np.logspace(-3,3,7),\n",
    "    'log_reg__max_iter'  : [100,1000,2500,5000]\n",
    "}\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.datacamp.com/tutorial/random-forests-classifier-python\n",
    "random_forest_params = [\n",
    "    {\"random_forest__n_estimators\": [100, 500],\n",
    "     \"random_forest__max_depth\" : [5, 10, 15]\n",
    "}\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gaussian naive bayes classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/code/akshaysharma001/naive-bayes-with-hyperpameter-tuning#Hyperparameter-Tuning-to-improve-Accuracy\n",
    "gnb_params = [\n",
    "    {'gnb__var_smoothing': np.logspace(0,-9, num=10)\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-means unsupervised classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Already use k-means++ and we set the no. clusters to no. of labels\n",
    "# Set params in loop\n",
    "kmeans_params = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Affinity propagation unsupervised classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://letsdatascience.com/affinity-propagation-clustering/\n",
    "affinity_propagation_params = [\n",
    "    {\"damping\": [0.5, 0.7]}, \n",
    "    {\"preference\": [-50,-10,0,10,50]}\n",
    "          ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train test loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "####################################################################################################\n",
      "Current dataset: acute_inflamations\n",
      "Dataset size: 120\n",
      "Labels in dataset: ['no' 'yes']\n",
      "####################################################################################################\n",
      "\n",
      "****************************************************************************************************\n",
      "Current seed: 41\n",
      "****************************************************************************************************\n",
      "\n",
      "____________________________________________________________________________________________________\n",
      "Classifier: knn\n",
      "Cross validation best parameters: {'knn__leaf_size': 15, 'knn__n_neighbors': 3, 'knn__weights': 'uniform'}\n",
      "knn, Train accuracy = 1.0, Test accuracy = 1.0\n",
      "knn, Train f1-score = 1.0, Test f1-score = 1.0\n",
      "____________________________________________________________________________________________________\n",
      "Classifier: svm\n",
      "Cross validation best parameters: {'svm__C': 0.1, 'svm__gamma': 0.1, 'svm__kernel': 'rbf'}\n",
      "svm, Train accuracy = 1.0, Test accuracy = 1.0\n",
      "svm, Train f1-score = 1.0, Test f1-score = 1.0\n",
      "____________________________________________________________________________________________________\n",
      "Classifier: log_reg\n",
      "Cross validation best parameters: {'log_reg__C': np.float64(0.1), 'log_reg__max_iter': 100, 'log_reg__penalty': 'l2'}\n",
      "log_reg, Train accuracy = 1.0, Test accuracy = 1.0\n",
      "log_reg, Train f1-score = 1.0, Test f1-score = 1.0\n",
      "____________________________________________________________________________________________________\n",
      "Classifier: random_forest\n",
      "Cross validation best parameters: {'random_forest__max_depth': 5, 'random_forest__n_estimators': 100}\n",
      "random_forest, Train accuracy = 1.0, Test accuracy = 1.0\n",
      "random_forest, Train f1-score = 1.0, Test f1-score = 1.0\n",
      "____________________________________________________________________________________________________\n",
      "Classifier: gnb\n",
      "Cross validation best parameters: {'gnb__var_smoothing': np.float64(1.0)}\n",
      "gnb, Train accuracy = 0.9791666666666666, Test accuracy = 1.0\n",
      "gnb, Train f1-score = 0.9790823211875844, Test f1-score = 1.0\n",
      "____________________________________________________________________________________________________\n",
      "Classifier: kmeans\n",
      "Cross validation best parameters:  {'n_clusters': 2, 'algorithm': 'lloyd'}\n",
      "kmeans, Train accuracy = 0.8125, Test accuracy = 0.875\n",
      "kmeans, Train f1-score = 0.7982630272952854, Test f1-score = 0.8700189753320683\n",
      "____________________________________________________________________________________________________\n",
      "Classifier: agglomerative_clustering\n",
      "Best score 0.8431578947368421 with params {'metric': 'euclidean', 'linkage': 'average', 'pca': True}\n",
      "agglo, Train accuracy = 0.8157894736842105, Test accuracy = 0.7916666666666666\n",
      "agglo, Train f1-score = 0.8026418988648091, Test f1-score = 0.7898550724637682\n",
      "____________________________________________________________________________________________________\n",
      "Classifier: affinity_propagation\n",
      "Cross validation best parameters:  {'preference': 0, 'damping': 0.5}\n",
      "affinity_propagation, Train accuracy = 1.0, Test accuracy = 1.0\n",
      "affinity_propagation, Train f1-score = 1.0, Test f1-score = 1.0\n",
      "****************************************************************************************************\n",
      "Current seed: 42\n",
      "****************************************************************************************************\n",
      "\n",
      "____________________________________________________________________________________________________\n",
      "Classifier: knn\n",
      "Cross validation best parameters: {'knn__leaf_size': 15, 'knn__n_neighbors': 3, 'knn__weights': 'uniform'}\n",
      "knn, Train accuracy = 1.0, Test accuracy = 1.0\n",
      "knn, Train f1-score = 1.0, Test f1-score = 1.0\n",
      "____________________________________________________________________________________________________\n",
      "Classifier: svm\n",
      "Cross validation best parameters: {'svm__C': 0.1, 'svm__gamma': 0.1, 'svm__kernel': 'rbf'}\n",
      "svm, Train accuracy = 1.0, Test accuracy = 1.0\n",
      "svm, Train f1-score = 1.0, Test f1-score = 1.0\n",
      "____________________________________________________________________________________________________\n",
      "Classifier: log_reg\n",
      "Cross validation best parameters: {'log_reg__C': np.float64(0.1), 'log_reg__max_iter': 100, 'log_reg__penalty': 'l2'}\n",
      "log_reg, Train accuracy = 1.0, Test accuracy = 1.0\n",
      "log_reg, Train f1-score = 1.0, Test f1-score = 1.0\n",
      "____________________________________________________________________________________________________\n",
      "Classifier: random_forest\n",
      "Cross validation best parameters: {'random_forest__max_depth': 5, 'random_forest__n_estimators': 100}\n",
      "random_forest, Train accuracy = 1.0, Test accuracy = 1.0\n",
      "random_forest, Train f1-score = 1.0, Test f1-score = 1.0\n",
      "____________________________________________________________________________________________________\n",
      "Classifier: gnb\n",
      "Cross validation best parameters: {'gnb__var_smoothing': np.float64(1.0)}\n",
      "gnb, Train accuracy = 1.0, Test accuracy = 1.0\n",
      "gnb, Train f1-score = 1.0, Test f1-score = 1.0\n",
      "____________________________________________________________________________________________________\n",
      "Classifier: kmeans\n",
      "Cross validation best parameters:  {'n_clusters': 2, 'algorithm': 'lloyd'}\n",
      "kmeans, Train accuracy = 0.7604166666666666, Test accuracy = 0.7083333333333334\n",
      "kmeans, Train f1-score = 0.756208865132177, Test f1-score = 0.6975308641975309\n",
      "____________________________________________________________________________________________________\n",
      "Classifier: agglomerative_clustering\n",
      "Best score 0.8305263157894738 with params {'metric': 'euclidean', 'linkage': 'ward', 'pca': True}\n",
      "agglo, Train accuracy = 0.5526315789473685, Test accuracy = 0.9166666666666666\n",
      "agglo, Train f1-score = 0.5589369830495902, Test f1-score = 0.9148148148148149\n",
      "____________________________________________________________________________________________________\n",
      "Classifier: affinity_propagation\n",
      "Cross validation best parameters:  {'preference': -50, 'damping': 0.7}\n",
      "affinity_propagation, Train accuracy = 1.0, Test accuracy = 1.0\n",
      "affinity_propagation, Train f1-score = 1.0, Test f1-score = 1.0\n",
      "****************************************************************************************************\n",
      "Current seed: 43\n",
      "****************************************************************************************************\n",
      "\n",
      "____________________________________________________________________________________________________\n",
      "Classifier: knn\n",
      "Cross validation best parameters: {'knn__leaf_size': 15, 'knn__n_neighbors': 3, 'knn__weights': 'uniform'}\n",
      "knn, Train accuracy = 1.0, Test accuracy = 1.0\n",
      "knn, Train f1-score = 1.0, Test f1-score = 1.0\n",
      "____________________________________________________________________________________________________\n",
      "Classifier: svm\n",
      "Cross validation best parameters: {'svm__C': 0.1, 'svm__gamma': 0.1, 'svm__kernel': 'rbf'}\n",
      "svm, Train accuracy = 1.0, Test accuracy = 1.0\n",
      "svm, Train f1-score = 1.0, Test f1-score = 1.0\n",
      "____________________________________________________________________________________________________\n",
      "Classifier: log_reg\n",
      "Cross validation best parameters: {'log_reg__C': np.float64(0.1), 'log_reg__max_iter': 100, 'log_reg__penalty': 'l2'}\n",
      "log_reg, Train accuracy = 1.0, Test accuracy = 1.0\n",
      "log_reg, Train f1-score = 1.0, Test f1-score = 1.0\n",
      "____________________________________________________________________________________________________\n",
      "Classifier: random_forest\n",
      "Cross validation best parameters: {'random_forest__max_depth': 5, 'random_forest__n_estimators': 100}\n",
      "random_forest, Train accuracy = 1.0, Test accuracy = 1.0\n",
      "random_forest, Train f1-score = 1.0, Test f1-score = 1.0\n",
      "____________________________________________________________________________________________________\n",
      "Classifier: gnb\n",
      "Cross validation best parameters: {'gnb__var_smoothing': np.float64(1.0)}\n",
      "gnb, Train accuracy = 0.96875, Test accuracy = 1.0\n",
      "gnb, Train f1-score = 0.9685488424618859, Test f1-score = 1.0\n",
      "____________________________________________________________________________________________________\n",
      "Classifier: kmeans\n",
      "Cross validation best parameters:  {'n_clusters': 2, 'algorithm': 'lloyd'}\n",
      "kmeans, Train accuracy = 0.5833333333333334, Test accuracy = 0.5833333333333334\n",
      "kmeans, Train f1-score = 0.4298245614035088, Test f1-score = 0.4298245614035088\n",
      "____________________________________________________________________________________________________\n",
      "Classifier: agglomerative_clustering\n",
      "Best score 0.8405263157894738 with params {'metric': 'euclidean', 'linkage': 'complete', 'pca': True}\n",
      "agglo, Train accuracy = 0.8026315789473685, Test accuracy = 0.875\n",
      "agglo, Train f1-score = 0.7867936137152868, Test f1-score = 0.8700189753320683\n",
      "____________________________________________________________________________________________________\n",
      "Classifier: affinity_propagation\n",
      "Cross validation best parameters:  {'preference': 0, 'damping': 0.5}\n",
      "affinity_propagation, Train accuracy = 1.0, Test accuracy = 1.0\n",
      "affinity_propagation, Train f1-score = 1.0, Test f1-score = 1.0\n",
      "****************************************************************************************************\n",
      "Current seed: 44\n",
      "****************************************************************************************************\n",
      "\n",
      "____________________________________________________________________________________________________\n",
      "Classifier: knn\n",
      "Cross validation best parameters: {'knn__leaf_size': 15, 'knn__n_neighbors': 3, 'knn__weights': 'uniform'}\n",
      "knn, Train accuracy = 1.0, Test accuracy = 1.0\n",
      "knn, Train f1-score = 1.0, Test f1-score = 1.0\n",
      "____________________________________________________________________________________________________\n",
      "Classifier: svm\n",
      "Cross validation best parameters: {'svm__C': 0.1, 'svm__gamma': 0.1, 'svm__kernel': 'rbf'}\n",
      "svm, Train accuracy = 1.0, Test accuracy = 1.0\n",
      "svm, Train f1-score = 1.0, Test f1-score = 1.0\n",
      "____________________________________________________________________________________________________\n",
      "Classifier: log_reg\n",
      "Cross validation best parameters: {'log_reg__C': np.float64(0.1), 'log_reg__max_iter': 100, 'log_reg__penalty': 'l2'}\n",
      "log_reg, Train accuracy = 1.0, Test accuracy = 1.0\n",
      "log_reg, Train f1-score = 1.0, Test f1-score = 1.0\n",
      "____________________________________________________________________________________________________\n",
      "Classifier: random_forest\n",
      "Cross validation best parameters: {'random_forest__max_depth': 5, 'random_forest__n_estimators': 100}\n",
      "random_forest, Train accuracy = 1.0, Test accuracy = 1.0\n",
      "random_forest, Train f1-score = 1.0, Test f1-score = 1.0\n",
      "____________________________________________________________________________________________________\n",
      "Classifier: gnb\n",
      "Cross validation best parameters: {'gnb__var_smoothing': np.float64(1.0)}\n",
      "gnb, Train accuracy = 1.0, Test accuracy = 1.0\n",
      "gnb, Train f1-score = 1.0, Test f1-score = 1.0\n",
      "____________________________________________________________________________________________________\n",
      "Classifier: kmeans\n",
      "Cross validation best parameters:  {'n_clusters': 2, 'algorithm': 'lloyd'}\n",
      "kmeans, Train accuracy = 0.5833333333333334, Test accuracy = 0.5833333333333334\n",
      "kmeans, Train f1-score = 0.4298245614035088, Test f1-score = 0.4298245614035088\n",
      "____________________________________________________________________________________________________\n",
      "Classifier: agglomerative_clustering\n",
      "Best score 0.8005263157894739 with params {'metric': 'euclidean', 'linkage': 'complete', 'pca': True}\n",
      "agglo, Train accuracy = 0.8026315789473685, Test accuracy = 0.9166666666666666\n",
      "agglo, Train f1-score = 0.7817872502891536, Test f1-score = 0.9148148148148149\n",
      "____________________________________________________________________________________________________\n",
      "Classifier: affinity_propagation\n",
      "Cross validation best parameters:  {'preference': 0, 'damping': 0.5}\n",
      "affinity_propagation, Train accuracy = 1.0, Test accuracy = 1.0\n",
      "affinity_propagation, Train f1-score = 1.0, Test f1-score = 1.0\n",
      "****************************************************************************************************\n",
      "Current seed: 45\n",
      "****************************************************************************************************\n",
      "\n",
      "____________________________________________________________________________________________________\n",
      "Classifier: knn\n",
      "Cross validation best parameters: {'knn__leaf_size': 15, 'knn__n_neighbors': 3, 'knn__weights': 'uniform'}\n",
      "knn, Train accuracy = 1.0, Test accuracy = 1.0\n",
      "knn, Train f1-score = 1.0, Test f1-score = 1.0\n",
      "____________________________________________________________________________________________________\n",
      "Classifier: svm\n",
      "Cross validation best parameters: {'svm__C': 0.1, 'svm__gamma': 0.1, 'svm__kernel': 'rbf'}\n",
      "svm, Train accuracy = 1.0, Test accuracy = 1.0\n",
      "svm, Train f1-score = 1.0, Test f1-score = 1.0\n",
      "____________________________________________________________________________________________________\n",
      "Classifier: log_reg\n",
      "Cross validation best parameters: {'log_reg__C': np.float64(0.1), 'log_reg__max_iter': 100, 'log_reg__penalty': 'l2'}\n",
      "log_reg, Train accuracy = 1.0, Test accuracy = 1.0\n",
      "log_reg, Train f1-score = 1.0, Test f1-score = 1.0\n",
      "____________________________________________________________________________________________________\n",
      "Classifier: random_forest\n",
      "Cross validation best parameters: {'random_forest__max_depth': 5, 'random_forest__n_estimators': 100}\n",
      "random_forest, Train accuracy = 1.0, Test accuracy = 1.0\n",
      "random_forest, Train f1-score = 1.0, Test f1-score = 1.0\n",
      "____________________________________________________________________________________________________\n",
      "Classifier: gnb\n",
      "Cross validation best parameters: {'gnb__var_smoothing': np.float64(1.0)}\n",
      "gnb, Train accuracy = 0.96875, Test accuracy = 1.0\n",
      "gnb, Train f1-score = 0.9685488424618859, Test f1-score = 1.0\n",
      "____________________________________________________________________________________________________\n",
      "Classifier: kmeans\n",
      "Cross validation best parameters:  {'n_clusters': 2, 'algorithm': 'lloyd'}\n",
      "kmeans, Train accuracy = 0.8333333333333334, Test accuracy = 0.7916666666666666\n",
      "kmeans, Train f1-score = 0.8229166666666666, Test f1-score = 0.7727272727272728\n",
      "____________________________________________________________________________________________________\n",
      "Classifier: agglomerative_clustering\n",
      "Best score 0.7389473684210526 with params {'metric': 'l1', 'linkage': 'single', 'pca': False}\n",
      "agglo, Train accuracy = 0.8947368421052632, Test accuracy = 0.6666666666666666\n",
      "agglo, Train f1-score = 0.8904437564499483, Test f1-score = 0.5925925925925926\n",
      "____________________________________________________________________________________________________\n",
      "Classifier: affinity_propagation\n",
      "Cross validation best parameters:  {'preference': -50, 'damping': 0.5}\n",
      "affinity_propagation, Train accuracy = 1.0, Test accuracy = 1.0\n",
      "affinity_propagation, Train f1-score = 1.0, Test f1-score = 1.0\n",
      "\n",
      "####################################################################################################\n",
      "Current dataset: balance_scale\n",
      "Dataset size: 625\n",
      "Labels in dataset: ['B' 'L' 'R']\n",
      "####################################################################################################\n",
      "\n",
      "****************************************************************************************************\n",
      "Current seed: 41\n",
      "****************************************************************************************************\n",
      "\n",
      "____________________________________________________________________________________________________\n",
      "Classifier: knn\n",
      "Cross validation best parameters: {'knn__leaf_size': 15, 'knn__n_neighbors': 9, 'knn__weights': 'distance'}\n",
      "knn, Train accuracy = 1.0, Test accuracy = 0.912\n",
      "knn, Train f1-score = 1.0, Test f1-score = 0.8741333333333334\n",
      "____________________________________________________________________________________________________\n",
      "Classifier: svm\n",
      "Cross validation best parameters: {'svm__C': 100, 'svm__gamma': 0.1, 'svm__kernel': 'rbf'}\n",
      "svm, Train accuracy = 1.0, Test accuracy = 0.984\n",
      "svm, Train f1-score = 1.0, Test f1-score = 0.984\n",
      "____________________________________________________________________________________________________\n",
      "Classifier: log_reg\n",
      "Cross validation best parameters: {'log_reg__C': np.float64(0.001), 'log_reg__max_iter': 100, 'log_reg__penalty': 'l2'}\n",
      "log_reg, Train accuracy = 0.88, Test accuracy = 0.888\n",
      "log_reg, Train f1-score = 0.8443403348237496, Test f1-score = 0.8512513368983957\n",
      "____________________________________________________________________________________________________\n",
      "Classifier: random_forest\n",
      "Cross validation best parameters: {'random_forest__max_depth': 5, 'random_forest__n_estimators': 500}\n",
      "random_forest, Train accuracy = 0.914, Test accuracy = 0.896\n",
      "random_forest, Train f1-score = 0.8798435702658708, Test f1-score = 0.8588354782608697\n",
      "____________________________________________________________________________________________________\n",
      "Classifier: gnb\n",
      "Cross validation best parameters: {'gnb__var_smoothing': np.float64(0.01)}\n",
      "gnb, Train accuracy = 0.904, Test accuracy = 0.912\n",
      "gnb, Train f1-score = 0.8673152628706567, Test f1-score = 0.874002916869227\n",
      "____________________________________________________________________________________________________\n",
      "Classifier: kmeans\n",
      "Cross validation best parameters:  {'n_clusters': 3, 'algorithm': 'lloyd'}\n",
      "kmeans, Train accuracy = 0.656, Test accuracy = 0.696\n",
      "kmeans, Train f1-score = 0.6230455422443576, Test f1-score = 0.6600034492670307\n",
      "____________________________________________________________________________________________________\n",
      "Classifier: agglomerative_clustering\n",
      "Best score 0.6085 with params {'metric': 'l1', 'linkage': 'average', 'pca': True}\n",
      "agglo, Train accuracy = 0.6025, Test accuracy = 0.712\n",
      "agglo, Train f1-score = 0.6359962958565404, Test f1-score = 0.7113041460257955\n",
      "____________________________________________________________________________________________________\n",
      "Classifier: affinity_propagation\n",
      "Cross validation best parameters:  {'preference': -50, 'damping': 0.5}\n",
      "affinity_propagation, Train accuracy = 0.742, Test accuracy = 0.784\n",
      "affinity_propagation, Train f1-score = 0.7117482517482517, Test f1-score = 0.7516190476190477\n",
      "****************************************************************************************************\n",
      "Current seed: 42\n",
      "****************************************************************************************************\n",
      "\n",
      "____________________________________________________________________________________________________\n",
      "Classifier: knn\n",
      "Cross validation best parameters: {'knn__leaf_size': 15, 'knn__n_neighbors': 9, 'knn__weights': 'distance'}\n",
      "knn, Train accuracy = 1.0, Test accuracy = 0.888\n",
      "knn, Train f1-score = 1.0, Test f1-score = 0.8587163947163947\n",
      "____________________________________________________________________________________________________\n",
      "Classifier: svm\n",
      "Cross validation best parameters: {'svm__C': 1000, 'svm__gamma': 0.01, 'svm__kernel': 'rbf'}\n",
      "svm, Train accuracy = 0.972, Test accuracy = 0.968\n",
      "svm, Train f1-score = 0.9734417862838916, Test f1-score = 0.9703834586466165\n",
      "____________________________________________________________________________________________________\n",
      "Classifier: log_reg\n",
      "Cross validation best parameters: {'log_reg__C': np.float64(100.0), 'log_reg__max_iter': 100, 'log_reg__penalty': 'l2'}\n",
      "log_reg, Train accuracy = 0.908, Test accuracy = 0.88\n",
      "log_reg, Train f1-score = 0.9129388054666053, Test f1-score = 0.8758727326791556\n",
      "____________________________________________________________________________________________________\n",
      "Classifier: random_forest\n",
      "Cross validation best parameters: {'random_forest__max_depth': 5, 'random_forest__n_estimators': 100}\n",
      "random_forest, Train accuracy = 0.914, Test accuracy = 0.824\n",
      "random_forest, Train f1-score = 0.885021936640765, Test f1-score = 0.7928959999999999\n",
      "____________________________________________________________________________________________________\n",
      "Classifier: gnb\n",
      "Cross validation best parameters: {'gnb__var_smoothing': np.float64(1.0)}\n",
      "gnb, Train accuracy = 0.906, Test accuracy = 0.888\n",
      "gnb, Train f1-score = 0.8693414078405891, Test f1-score = 0.851000208347802\n",
      "____________________________________________________________________________________________________\n",
      "Classifier: kmeans\n",
      "Cross validation best parameters:  {'n_clusters': 3, 'algorithm': 'lloyd'}\n",
      "kmeans, Train accuracy = 0.74, Test accuracy = 0.744\n",
      "kmeans, Train f1-score = 0.7076584163778162, Test f1-score = 0.7104000000000001\n",
      "____________________________________________________________________________________________________\n",
      "Classifier: agglomerative_clustering\n",
      "Best score 0.618 with params {'metric': 'l1', 'linkage': 'average', 'pca': False}\n",
      "agglo, Train accuracy = 0.65, Test accuracy = 0.496\n",
      "agglo, Train f1-score = 0.693780972787658, Test f1-score = 0.49242927967337413\n",
      "____________________________________________________________________________________________________\n",
      "Classifier: affinity_propagation\n",
      "Cross validation best parameters:  {'preference': -10, 'damping': 0.7}\n",
      "affinity_propagation, Train accuracy = 0.8, Test accuracy = 0.744\n",
      "affinity_propagation, Train f1-score = 0.7674126393404046, Test f1-score = 0.712676401453095\n",
      "****************************************************************************************************\n",
      "Current seed: 43\n",
      "****************************************************************************************************\n",
      "\n",
      "____________________________________________________________________________________________________\n",
      "Classifier: knn\n",
      "Cross validation best parameters: {'knn__leaf_size': 15, 'knn__n_neighbors': 7, 'knn__weights': 'distance'}\n",
      "knn, Train accuracy = 1.0, Test accuracy = 0.88\n",
      "knn, Train f1-score = 1.0, Test f1-score = 0.8582640714951478\n",
      "____________________________________________________________________________________________________\n",
      "Classifier: svm\n",
      "Cross validation best parameters: {'svm__C': 1000, 'svm__gamma': 0.1, 'svm__kernel': 'rbf'}\n",
      "svm, Train accuracy = 1.0, Test accuracy = 0.992\n",
      "svm, Train f1-score = 1.0, Test f1-score = 0.9921556935817805\n",
      "____________________________________________________________________________________________________\n",
      "Classifier: log_reg\n",
      "Cross validation best parameters: {'log_reg__C': np.float64(10.0), 'log_reg__max_iter': 100, 'log_reg__penalty': 'l2'}\n",
      "log_reg, Train accuracy = 0.898, Test accuracy = 0.88\n",
      "log_reg, Train f1-score = 0.8926960737712711, Test f1-score = 0.8717449737743291\n",
      "____________________________________________________________________________________________________\n",
      "Classifier: random_forest\n",
      "Cross validation best parameters: {'random_forest__max_depth': 5, 'random_forest__n_estimators': 100}\n",
      "random_forest, Train accuracy = 0.908, Test accuracy = 0.872\n",
      "random_forest, Train f1-score = 0.8711661122661124, Test f1-score = 0.8358095238095239\n",
      "____________________________________________________________________________________________________\n",
      "Classifier: gnb\n",
      "Cross validation best parameters: {'gnb__var_smoothing': np.float64(0.1)}\n",
      "gnb, Train accuracy = 0.906, Test accuracy = 0.912\n",
      "gnb, Train f1-score = 0.8693002365295143, Test f1-score = 0.8743851656365025\n",
      "____________________________________________________________________________________________________\n",
      "Classifier: kmeans\n",
      "Cross validation best parameters:  {'n_clusters': 3, 'algorithm': 'lloyd'}\n",
      "kmeans, Train accuracy = 0.64, Test accuracy = 0.648\n",
      "kmeans, Train f1-score = 0.6073815163967449, Test f1-score = 0.6146537717601548\n",
      "____________________________________________________________________________________________________\n",
      "Classifier: agglomerative_clustering\n",
      "Best score 0.525 with params {'metric': 'euclidean', 'linkage': 'average', 'pca': False}\n",
      "agglo, Train accuracy = 0.425, Test accuracy = 0.552\n",
      "agglo, Train f1-score = 0.442259725506393, Test f1-score = 0.4764190476190477\n",
      "____________________________________________________________________________________________________\n",
      "Classifier: affinity_propagation\n",
      "Cross validation best parameters:  {'preference': -10, 'damping': 0.7}\n",
      "affinity_propagation, Train accuracy = 0.772, Test accuracy = 0.76\n",
      "affinity_propagation, Train f1-score = 0.7410936660268714, Test f1-score = 0.7277858196894105\n",
      "****************************************************************************************************\n",
      "Current seed: 44\n",
      "****************************************************************************************************\n",
      "\n",
      "____________________________________________________________________________________________________\n",
      "Classifier: knn\n",
      "Cross validation best parameters: {'knn__leaf_size': 15, 'knn__n_neighbors': 9, 'knn__weights': 'distance'}\n",
      "knn, Train accuracy = 1.0, Test accuracy = 0.904\n",
      "knn, Train f1-score = 1.0, Test f1-score = 0.8662666666666666\n",
      "____________________________________________________________________________________________________\n",
      "Classifier: svm\n",
      "Cross validation best parameters: {'svm__C': 1000, 'svm__gamma': 0.1, 'svm__kernel': 'rbf'}\n",
      "svm, Train accuracy = 1.0, Test accuracy = 0.992\n",
      "svm, Train f1-score = 1.0, Test f1-score = 0.9921556935817805\n",
      "____________________________________________________________________________________________________\n",
      "Classifier: log_reg\n",
      "Cross validation best parameters: {'log_reg__C': np.float64(100.0), 'log_reg__max_iter': 100, 'log_reg__penalty': 'l2'}\n",
      "log_reg, Train accuracy = 0.9, Test accuracy = 0.904\n",
      "log_reg, Train f1-score = 0.8987937764972217, Test f1-score = 0.9032043622569939\n",
      "____________________________________________________________________________________________________\n",
      "Classifier: random_forest\n",
      "Cross validation best parameters: {'random_forest__max_depth': 5, 'random_forest__n_estimators': 100}\n",
      "random_forest, Train accuracy = 0.908, Test accuracy = 0.872\n",
      "random_forest, Train f1-score = 0.8740375092773841, Test f1-score = 0.8357165879410947\n",
      "____________________________________________________________________________________________________\n",
      "Classifier: gnb\n",
      "Cross validation best parameters: {'gnb__var_smoothing': np.float64(0.1)}\n",
      "gnb, Train accuracy = 0.904, Test accuracy = 0.896\n",
      "gnb, Train f1-score = 0.8673121968121968, Test f1-score = 0.8593904523660622\n",
      "____________________________________________________________________________________________________\n",
      "Classifier: kmeans\n",
      "Cross validation best parameters:  {'n_clusters': 3, 'algorithm': 'elkan'}\n",
      "kmeans, Train accuracy = 0.634, Test accuracy = 0.704\n",
      "kmeans, Train f1-score = 0.5975519001701646, Test f1-score = 0.6724615384615384\n",
      "____________________________________________________________________________________________________\n",
      "Classifier: agglomerative_clustering\n",
      "Best score 0.543 with params {'metric': 'euclidean', 'linkage': 'complete', 'pca': False}\n",
      "agglo, Train accuracy = 0.475, Test accuracy = 0.568\n",
      "agglo, Train f1-score = 0.5022397002663818, Test f1-score = 0.5642276422764227\n",
      "____________________________________________________________________________________________________\n",
      "Classifier: affinity_propagation\n",
      "Cross validation best parameters:  {'preference': -10, 'damping': 0.7}\n",
      "affinity_propagation, Train accuracy = 0.794, Test accuracy = 0.792\n",
      "affinity_propagation, Train f1-score = 0.7617884750344397, Test f1-score = 0.758917477076966\n",
      "****************************************************************************************************\n",
      "Current seed: 45\n",
      "****************************************************************************************************\n",
      "\n",
      "____________________________________________________________________________________________________\n",
      "Classifier: knn\n",
      "Cross validation best parameters: {'knn__leaf_size': 15, 'knn__n_neighbors': 9, 'knn__weights': 'uniform'}\n",
      "knn, Train accuracy = 0.912, Test accuracy = 0.888\n",
      "knn, Train f1-score = 0.8905784248345819, Test f1-score = 0.8512258064516129\n",
      "____________________________________________________________________________________________________\n",
      "Classifier: svm\n",
      "Cross validation best parameters: {'svm__C': 1000, 'svm__gamma': 0.01, 'svm__kernel': 'rbf'}\n",
      "svm, Train accuracy = 0.978, Test accuracy = 0.968\n",
      "svm, Train f1-score = 0.9788206578491047, Test f1-score = 0.9693147993983701\n",
      "____________________________________________________________________________________________________\n",
      "Classifier: log_reg\n",
      "Cross validation best parameters: {'log_reg__C': np.float64(100.0), 'log_reg__max_iter': 100, 'log_reg__penalty': 'l2'}\n",
      "log_reg, Train accuracy = 0.914, Test accuracy = 0.888\n",
      "log_reg, Train f1-score = 0.9220408270068442, Test f1-score = 0.8948696741854636\n",
      "____________________________________________________________________________________________________\n",
      "Classifier: random_forest\n",
      "Cross validation best parameters: {'random_forest__max_depth': 5, 'random_forest__n_estimators': 100}\n",
      "random_forest, Train accuracy = 0.916, Test accuracy = 0.864\n",
      "random_forest, Train f1-score = 0.8844370219447908, Test f1-score = 0.8276639566395663\n",
      "____________________________________________________________________________________________________\n",
      "Classifier: gnb\n",
      "Cross validation best parameters: {'gnb__var_smoothing': np.float64(0.01)}\n",
      "gnb, Train accuracy = 0.91, Test accuracy = 0.904\n",
      "gnb, Train f1-score = 0.8732797130082146, Test f1-score = 0.8665868463087715\n",
      "____________________________________________________________________________________________________\n",
      "Classifier: kmeans\n",
      "Cross validation best parameters:  {'n_clusters': 3, 'algorithm': 'lloyd'}\n",
      "kmeans, Train accuracy = 0.658, Test accuracy = 0.672\n",
      "kmeans, Train f1-score = 0.6231073089791367, Test f1-score = 0.6314406580493537\n",
      "____________________________________________________________________________________________________\n",
      "Classifier: agglomerative_clustering\n",
      "Best score 0.646 with params {'metric': 'l1', 'linkage': 'average', 'pca': False}\n",
      "agglo, Train accuracy = 0.51, Test accuracy = 0.568\n",
      "agglo, Train f1-score = 0.5321325811757567, Test f1-score = 0.6132754082765182\n",
      "____________________________________________________________________________________________________\n",
      "Classifier: affinity_propagation\n",
      "Cross validation best parameters:  {'preference': -10, 'damping': 0.7}\n",
      "affinity_propagation, Train accuracy = 0.778, Test accuracy = 0.808\n",
      "affinity_propagation, Train f1-score = 0.7464525016244314, Test f1-score = 0.7744\n",
      "\n",
      "####################################################################################################\n",
      "Current dataset: balloons\n",
      "Dataset size: 16\n",
      "Labels in dataset: ['F' 'T']\n",
      "####################################################################################################\n",
      "\n",
      "****************************************************************************************************\n",
      "Current seed: 41\n",
      "****************************************************************************************************\n",
      "\n",
      "____________________________________________________________________________________________________\n",
      "Classifier: knn\n",
      "Cross validation best parameters: {'knn__leaf_size': 15, 'knn__n_neighbors': 9, 'knn__weights': 'distance'}\n",
      "knn, Train accuracy = 1.0, Test accuracy = 0.25\n",
      "knn, Train f1-score = 1.0, Test f1-score = 0.2\n",
      "____________________________________________________________________________________________________\n",
      "Classifier: svm\n",
      "Cross validation best parameters: {'svm__C': 10, 'svm__gamma': 0.1, 'svm__kernel': 'rbf'}\n",
      "svm, Train accuracy = 1.0, Test accuracy = 1.0\n",
      "svm, Train f1-score = 1.0, Test f1-score = 1.0\n",
      "____________________________________________________________________________________________________\n",
      "Classifier: log_reg\n",
      "Cross validation best parameters: {'log_reg__C': np.float64(0.1), 'log_reg__max_iter': 100, 'log_reg__penalty': 'l2'}\n",
      "log_reg, Train accuracy = 0.8333333333333334, Test accuracy = 0.5\n",
      "log_reg, Train f1-score = 0.8229166666666666, Test f1-score = 0.5\n",
      "____________________________________________________________________________________________________\n",
      "Classifier: random_forest\n"
     ]
    }
   ],
   "source": [
    "for i, (name, id) in enumerate(data_set_sorted.items()):\n",
    "        print(\"\\n\" + \"#\"*100)\n",
    "        print(f\"Current dataset: {name}\")\n",
    "        ordinal_encoder = OrdinalEncoder()\n",
    "\n",
    "        # Set up dataset\n",
    "        X, y = import_dataset(dataset_id[name], ordinal_encoder)\n",
    "        labels = np.unique(y)\n",
    "        y = encode_categorical_features(y, ordinal_encoder)\n",
    "        print(f\"Dataset size: {len(X)}\")\n",
    "        print(f\"Labels in dataset: {labels}\")\n",
    "        print(\"#\"*100, end=\"\\n\\n\")\n",
    "\n",
    "\n",
    "        for RANDOM_SEED in RANDOM_SEEDS:   \n",
    "\n",
    "                print(\"*\"*100)\n",
    "                print(\"Current seed:\", RANDOM_SEED)\n",
    "                print(\"*\"*100, end=\"\\n\\n\")\n",
    "\n",
    "                # split the dataset\n",
    "                X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=TEST_SIZE, random_state=RANDOM_SEED, stratify = y)\n",
    "\n",
    "                # KNN\n",
    "                clf_name = \"knn\"\n",
    "                print(\"_\"*100)\n",
    "                print(f\"Classifier: {clf_name}\")\n",
    "                # https://towardsdatascience.com/gridsearchcv-for-beginners-db48a90114ee\n",
    "                knn = KNeighborsClassifier()\n",
    "                train_acc, train_f1, test_acc, test_f1, cm_train, cm_test = test_classifier(knn, clf_name, knn_params, X_train, y_train, X_test, y_test)\n",
    "\n",
    "                write(writer, name, clf_name, cm_train, \"train\", id[1] )\n",
    "                write(writer, name, clf_name, cm_test, \"test\", id[1] )\n",
    "\n",
    "                metrics_dict = save_metrics_to_dict(clf_name,\n",
    "                                                train_acc, \n",
    "                                                train_f1, \n",
    "                                                test_acc, \n",
    "                                                test_f1, \n",
    "                                                metrics_dict,\n",
    "                                                i,\n",
    "                                                RANDOM_SEED\n",
    "                                                )\n",
    "                \n",
    "                # SVM \n",
    "                svm = SVC()\n",
    "                clf_name = \"svm\"\n",
    "                print(\"_\"*100)\n",
    "                print(f\"Classifier: {clf_name}\")\n",
    "                train_acc, train_f1, test_acc, test_f1, cm_train, cm_test = test_classifier(svm, clf_name, svm_params, X_train, y_train, X_test, y_test)\n",
    "                write(writer, name, clf_name, cm_train, \"train\", id[1] )\n",
    "                write(writer, name, clf_name, cm_test, \"test\", id[1] )\n",
    "                metrics_dict = save_metrics_to_dict(clf_name,\n",
    "                                                train_acc, \n",
    "                                                train_f1, \n",
    "                                                test_acc, \n",
    "                                                test_f1, \n",
    "                                                metrics_dict,\n",
    "                                                i,\n",
    "                                                RANDOM_SEED\n",
    "                                                )\n",
    "\n",
    "                # Logistic regression\n",
    "                log_reg = LogisticRegression()\n",
    "                clf_name = \"log_reg\"\n",
    "                print(\"_\"*100)\n",
    "                print(f\"Classifier: {clf_name}\")\n",
    "                train_acc, train_f1, test_acc, test_f1, cm_train, cm_test = test_classifier(log_reg, \n",
    "                                                                                        clf_name, \n",
    "                                                                                        log_reg_params, \n",
    "                                                                                        X_train, \n",
    "                                                                                        y_train, \n",
    "                                                                                        X_test, \n",
    "                                                                                        y_test)\n",
    "                write(writer, name, clf_name, cm_train, \"train\", id[1] )\n",
    "                write(writer, name, clf_name, cm_test, \"test\", id[1] )\n",
    "                metrics_dict = save_metrics_to_dict(clf_name,\n",
    "                                                train_acc, \n",
    "                                                train_f1, \n",
    "                                                test_acc, \n",
    "                                                test_f1, \n",
    "                                                metrics_dict,\n",
    "                                                i,\n",
    "                                                RANDOM_SEED\n",
    "                                                )\n",
    "                \n",
    "                # Random forest\n",
    "                random_forest = RandomForestClassifier()\n",
    "                clf_name = \"random_forest\"\n",
    "                print(\"_\"*100)\n",
    "                print(f\"Classifier: {clf_name}\")\n",
    "                train_acc, train_f1, test_acc, test_f1, cm_train, cm_test = test_classifier(random_forest, \n",
    "                                                                                        clf_name, \n",
    "                                                                                        random_forest_params, \n",
    "                                                                                        X_train, \n",
    "                                                                                        y_train, \n",
    "                                                                                        X_test, \n",
    "                                                                                        y_test)\n",
    "                write(writer, name, clf_name, cm_train, \"train\", id[1] )\n",
    "                write(writer, name, clf_name, cm_test, \"test\", id[1] )\n",
    "                metrics_dict = save_metrics_to_dict(clf_name,\n",
    "                                                train_acc, \n",
    "                                                train_f1, \n",
    "                                                test_acc, \n",
    "                                                test_f1, \n",
    "                                                metrics_dict,\n",
    "                                                i,\n",
    "                                                RANDOM_SEED\n",
    "                                                )\n",
    "                \n",
    "                # Gaussian naive bayes\n",
    "                gnb = GaussianNB()\n",
    "                clf_name = \"gnb\"\n",
    "                print(\"_\"*100)\n",
    "                print(f\"Classifier: {clf_name}\")\n",
    "                train_acc, train_f1, test_acc, test_f1, cm_train, cm_test = test_classifier(gnb, \n",
    "                                                                                        clf_name, \n",
    "                                                                                        gnb_params, \n",
    "                                                                                        X_train, \n",
    "                                                                                        y_train, \n",
    "                                                                                        X_test, \n",
    "                                                                                        y_test)\n",
    "                write(writer, name, clf_name, cm_train, \"train\", id[1] )\n",
    "                write(writer, name, clf_name, cm_test, \"test\", id[1] )\n",
    "                metrics_dict = save_metrics_to_dict(clf_name,\n",
    "                                                train_acc, \n",
    "                                                train_f1, \n",
    "                                                test_acc, \n",
    "                                                test_f1, \n",
    "                                                metrics_dict,\n",
    "                                                i,\n",
    "                                                RANDOM_SEED\n",
    "                                                )\n",
    "                \n",
    "        \n",
    "                # K-means\n",
    "                clf_name = \"kmeans\"\n",
    "                print(\"_\"*100)\n",
    "                print(f\"Classifier: {clf_name}\")\n",
    "                kmeans_params = [{\"algorithm\": [\"lloyd\", \"elkan\"]}]\n",
    "                n_clusters = len(labels)\n",
    "                kmeans_params.append({\"n_clusters\": [n_clusters]})\n",
    "                train_acc, train_f1, test_acc, test_f1, cm_train, cm_test = clustering_classification(KMeans, clf_name, kmeans_params, X_train, y_train, X_test, y_test, RANDOM_SEED, K_FOLDS)\n",
    "                write(writer, name, clf_name, cm_train, \"train\", id[1])\n",
    "                write(writer, name, clf_name, cm_test, \"test\", id[1] )\n",
    "                metrics_dict = save_metrics_to_dict(clf_name,\n",
    "                                                train_acc, \n",
    "                                                train_f1, \n",
    "                                                test_acc, \n",
    "                                                test_f1, \n",
    "                                                metrics_dict,\n",
    "                                                i,\n",
    "                                                RANDOM_SEED\n",
    "                                                )\n",
    "\n",
    "                # Agglomerative clustering\n",
    "                clf_name = \"agglomerative_clustering\"\n",
    "                print(\"_\"*100)\n",
    "                print(f\"Classifier: {clf_name}\")\n",
    "                train_acc, train_f1, test_acc, test_f1, cm_train, cm_test = agg_clustering(X_train, y_train, X_test, y_test, RANDOM_SEED)\n",
    "                write(writer, name, clf_name, cm_train, \"train\", id[1] )\n",
    "                write(writer, name, clf_name, cm_test, \"test\", id[1] )\n",
    "                metrics_dict = save_metrics_to_dict(clf_name,\n",
    "                                                train_acc, \n",
    "                                                train_f1, \n",
    "                                                test_acc, \n",
    "                                                test_f1, \n",
    "                                                metrics_dict,\n",
    "                                                i,\n",
    "                                                RANDOM_SEED\n",
    "                                                )\n",
    "                \n",
    "\n",
    "                # Affinity propagation\n",
    "                clf_name = \"affinity_propagation\"\n",
    "                print(\"_\"*100)\n",
    "                print(f\"Classifier: {clf_name}\")\n",
    "                train_acc, train_f1, test_acc, test_f1, cm_train, cm_test =  clustering_classification(AffinityPropagation, clf_name, affinity_propagation_params, X_train, y_train, X_test, y_test, RANDOM_SEED, K_FOLDS)\n",
    "                write(writer, name, clf_name, cm_train, \"train\", id[1] )\n",
    "                write(writer, name, clf_name, cm_test, \"test\", id[1] )\n",
    "                metrics_dict = save_metrics_to_dict(clf_name,\n",
    "                                                train_acc, \n",
    "                                                train_f1, \n",
    "                                                test_acc, \n",
    "                                                test_f1, \n",
    "                                                metrics_dict,\n",
    "                                                i,\n",
    "                                                RANDOM_SEED\n",
    "                                                )\n",
    "\n",
    "        # Take average\n",
    "        keys_to_update = [\"train_acc_dict\", \"test_acc_dict\", \"train_f1_dict\", \"test_f1_dict\"]\n",
    "        for key in keys_to_update:\n",
    "                metrics_dict[key] = {k: v / len(RANDOM_SEEDS) for k, v in metrics_dict[key].items()}\n",
    "        # Write metrics to tensorboard, step is dataset id      \n",
    "        writer.add_scalars(\"Train accuracy\", metrics_dict[\"train_acc_dict\"], id[1])\n",
    "        writer.add_scalars(\"Test accuracy\", metrics_dict[\"test_acc_dict\"], id[1])\n",
    "        writer.add_scalars(\"Train f1\",  metrics_dict[\"train_f1_dict\"], id[1])\n",
    "        writer.add_scalars(\"Test f1\", metrics_dict[\"test_f1_dict\"], id[1])\n",
    "        # writer.add_scalars(\"Train average accuracy\", metrics_dict[\"train_acc_avg\"], id[1])\n",
    "        # writer.add_scalars(\"Test average accuracy\", metrics_dict[\"test_acc_avg\"], id[1])\n",
    "        # writer.add_scalars(\"Train average f1\", metrics_dict[\"train_f1_avg\"], id[1])\n",
    "        # writer.add_scalars(\"Test average f1\", metrics_dict[\"test_f1_avg\"], id[1])\n",
    "        writer_add_scalars(\"Train average accuracy\", writer, metrics_dict[\"train_acc_avg\"], id[1])\n",
    "        writer_add_scalars(\"Test average accuracy\", writer, metrics_dict[\"test_acc_avg\"], id[1])\n",
    "        writer_add_scalars(\"Train average f1\", writer, metrics_dict[\"train_f1_avg\"], id[1])\n",
    "        writer_add_scalars(\"Test average f1\", writer, metrics_dict[\"test_f1_avg\"], id[1])\n",
    "        \n",
    "        # reset dictionaries\n",
    "        metrics_dict[\"train_acc_dict\"] = {}\n",
    "        metrics_dict[\"train_f1_dict\"] = {}\n",
    "        metrics_dict[\"test_acc_dict\"] = {}\n",
    "        metrics_dict[\"test_f1_dict\"] = {}\n",
    "        \n",
    "\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rank  Classifier                     Avg. test acc.    Avg. train acc.   Avg. test F1      Avg train F1     \n",
      "____________________________________________________________________________________________________\n",
      "1     svm                            0.842             0.920             0.825             0.908            \n",
      "2     random_forest                  0.836             0.958             0.821             0.954            \n",
      "3     knn                            0.813             0.930             0.801             0.925            \n",
      "4     log_reg                        0.810             0.846             0.786             0.826            \n",
      "5     gnb                            0.779             0.818             0.763             0.807            \n",
      "6     affinity_propagation           0.768             0.804             0.736             0.774            \n",
      "7     kmeans                         0.718             0.730             0.653             0.667            \n",
      "8     agglomerative_clustering       0.710             0.725             0.671             0.672            \n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "rank_counter = 0\n",
    "print(f\"{'Rank':<5} {'Classifier':<30} {'Avg. test acc.':<17} {'Avg. train acc.':<17} {'Avg. test F1':<17} {'Avg train F1':<17}\")\n",
    "print(\"_\"*100)\n",
    "for i, clf in enumerate(sorted(metrics_dict[\"test_acc_avg\"].items(), key=lambda x: x[1], reverse=True)):\n",
    "    if clf[0].endswith(\"count\"):\n",
    "        rank_counter += 1\n",
    "        continue\n",
    "    print(f\"{i+1-rank_counter:<5} {clf[0][:-4]:<30} {clf[1]:<17.3f} {metrics_dict['train_acc_avg'][clf[0]]:<8.3f}\\\n",
    "          {metrics_dict['test_f1_avg'][clf[0]]:<18.3f}{metrics_dict['train_f1_avg'][clf[0]]:<17.3f}\")\n",
    "print(\"_\"*100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AppliedAILabs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
